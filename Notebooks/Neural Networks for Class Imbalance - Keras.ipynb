{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks for Class Imbalance with Keras\n",
    "\n",
    "Kaggle CreditCard Fraud Detection Data can be downloaded here:\n",
    "https://github.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/blob/master/creditcard.csv?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test splits\n",
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# retrieve numpy array\n",
    "data = data.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, 1:-1], data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unique values of each class\n",
    "\n",
    "import numpy as np\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The class weighing can be defined multiple ways; for example:\n",
    "\n",
    "* Domain expertise, determined by talking to subject matter experts\n",
    "* Tuning, determined by a hyperparameter search such as a grid search\n",
    "* Heuristic, specified using a general best practice\n",
    "* A best practice for using the class weighting is to use the inverse of the class distribution present in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate heuristic class weighting\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# calculate class weighting according to training data\n",
    "weighting = compute_class_weight('balanced', [0,1], y_train)\n",
    "print(weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Neural Netoworks:\n",
    "\n",
    "A large error weighting can be applied to those examples in the minority class as they are often more important in an imbalanced classification problem than examples from the majority class.\n",
    "\n",
    "* *Large Weight:* Assigned to examples from the minority class.\n",
    "* *Small Weight:* Assigned to examples from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "weights = {0:weighting[0], 1:weighting[1]} \n",
    "print(weights)\n",
    "# define model\n",
    "# try with class_weight=weights and class_weight=None\n",
    "\n",
    "# the number of input features\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "# define the neural network model\n",
    "# define model\n",
    "model = Sequential()\n",
    "# define first hidden layer and visible layer\n",
    "model.add(Dense(10, input_dim=n_input, activation='relu', kernel_initializer='he_uniform'))\n",
    "# define output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# define loss and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, class_weight=weights, epochs=100, verbose=0)\n",
    "\n",
    "# evaluate model\n",
    "y_probs = model.predict(X_test)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# summarize performance\n",
    "print(' ROC AUC = %.3f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# prepare train and test dataset\n",
    "def prepare_data(n_samples=1000):\n",
    "    # generate 2d classification dataset\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n",
    "    n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
    "    # split into train and test\n",
    "    n_train = n_samples//2\n",
    "    trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "    trainy, testy = y[:n_train], y[n_train:]\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# define the neural network model\n",
    "def define_model(n_input):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # define first hidden layer and visible layer\n",
    "    model.add(Dense(10, input_dim=n_input, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # define output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # define loss and optimizer\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "    return model\n",
    "\n",
    "# prepare dataset\n",
    "X_train, y_train, X_test, y_test = prepare_data()\n",
    "# get the model\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "model = define_model(n_input)\n",
    "# fit model\n",
    "# calculate class weighting according to training data\n",
    "weighting = compute_class_weight('balanced', [0,1], y_train)\n",
    "\n",
    "weights = {0:weighting[0], 1:weighting[1]} \n",
    "print(weights)\n",
    "\n",
    "history = model.fit(X_train, y_train, class_weight=weights, epochs=100, verbose=0)\n",
    "# evaluate model\n",
    "yhat = model.predict(X_test)\n",
    "score = roc_auc_score(y_test, yhat)\n",
    "print('ROC AUC: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
